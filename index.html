<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>MoMA Personalization</title>
<link href="./DreamBooth_files/style.css" rel="stylesheet">
<script type="text/javascript" src="./DreamBooth_files/jquery.mlens-1.0.min.js"></script> 
<script type="text/javascript" src="./DreamBooth_files/jquery.js"></script>
</head>

<body>
<div class="content">
  <!-- <h1>MoMA: Multimodal LLM Adapter for Fast Personalized Image Generation</h1> -->
  <h1><span style="color: red;">MoMA</span>: <strong><span style="color: red;">M</span>ultim<span style="color: red;">o</span>dal LL<span style="color: red;">M</span> <span style="color: red;">A</span>dapter for Fast Personalized</strong></h1>
  <h1><strong>Image Generation</strong></h1>
  <p id="authors"><span><a href="https://natanielruiz.github.io/"></a></span>Kunpeng Song<sup>1,</sup><sup>2</sup>,    Yizhe zhu<sup>1</sup>,   Bingchen Liu<sup>1</sup>,   Qing Yan<sup>1</sup>,    Ahmed Elgammal<sup>2</sup>    and Xiao Yang<sup>1</sup><br>
    <br>
  <span style="color: rgb(0, 85, 150);font-size: 20px" class="author-block"><sup>1</sup>ByteDance</span>
  <span style="color: rgb(0, 85, 150);font-size: 20px" class="author-block"><sup>2</sup>Rutgers University</span>
  </p>
  <br>
  <img src="./DreamBooth_files/hero_web.jpg" class="teaser-gif" style="width:100%;"><br>
    <font size="+2">
          <p style="text-align: center;">
            <a href="https://drive.google.com/file/d/1i5gmq4ZsPdDJqNvm9KQUpTSWu-RqudLP/view?usp=sharing" target="_blank">[Paper]</a> &nbsp;&nbsp;&nbsp;&nbsp;
            [Code](coming soon) &nbsp;&nbsp;&nbsp;&nbsp;
          </p>
    </font>
</div>
<div class="content">
  <h2 style="text-align:center;">Abstract</h2>
  <p>In this paper, we present MoMA: an open-vocabulary, training-free personalized image model that boasts flexible zero-shot capabilities. As foundational text-to-image models rapidly evolve, the demand for robust image-to-image translation grows. Addressing this need, MoMA specializes in subject-driven personalized image generation. Utilizing an open-source, Multimodal Large Language Model (MLLM), we train MoMA to serve a dual role as both a feature extractor and a generator. This approach effectively synergizes reference image and text prompt information to produce valuable image features, facilitating an image diffusion model. To better leverage the generated features, we further introduce a novel self-attention shortcut method that efficiently transfers image features to an image diffusion model, improving the resemblance of the target object in generated images. Remarkably, as a tuning-free plug-and-play module, our model requires only a single reference image and outperforms existing methods in generating images with high detail fidelity, enhanced identity-preservation and prompt faithfulness. We commit to making our work open-source, thereby providing universal access to these advancements.</p>
</div>
<div class="content">
  <h2>Approach</h2>
  <p> We present MoMA, a multimodal LLM adapter enhanced by fine-grained feature transfer. The overall architecture is demonstrated in Figure below. Our method consists of three parts: (1) a generative multimodal decoder is utilized to extract image features from the reference image and edit it following the target prompt, yield the contextualized image feature; (2) in the meantime, we replace the back ground of the original image by white color, leaving only object pixels, leveraging the original UNetâ€™s self-attention layers to extract the object image feature; finally, during the new image generation process, we injected the contextualized image features and the object image features into the UNet diffusion model with the dedicatedly trained context-cross-attention layers and object-cross-attention layers, respectively.</p>
  <br>
  <img class="summary-img" src="./DreamBooth_files/2.png" style="width:100%;"> <br>
  <p>To achieve the best model performance, we propose a two-staged pre-training strategy. First, we propose a Multimodal Generative Learning Stage, where we pre-train the multimodal image-feature decoder such that it learns to compose image features of the subject with the target prompt and output the CLIP embedding of the target image. Second, subject and context cross attention layers are trained to inject this embedding. To further enhance the detail faithfulness, we involve image self-attention features transfer and apply a masking mechanism</p>
  <br>
  <img class="summary-img" src="./DreamBooth_files/3.png" style="width:100%;"> <br>
</div>
<div class="content">
  <h2>Results for context editing</h2>
  <p>We present qualitative examples to illustrate the effectiveness of our model. In Figure below, the target prompts specify a novel contextual environment. Our model seamlessly generates a high-quality background while precisely situating the same object within this new setting </p>
<img class="summary-img" src="./DreamBooth_files/4.png" style="width:100%;">
</div>
<div class="content">
  <h2>Results for texture editing</h2>
  <p>In the following image, the prompts indicate a change in texture. Our model showcases its ability to render realistic textures in response to the textual cues, adeptly altering specified visual elements while leaving other identity aspects of the image unaffected..</p>
  <br>
  <img class="summary-img" src="./DreamBooth_files/texture.png" style="width:100%;"> <br>
</div>
<div class="content">
  <h2>Compare with other methods</h2>
  <p>Zero-shot Qualitative Comparison. We share recontextualization in upper panel and texture editing in lower panel. Our results have significantly more accurate details for context editing and better balancing between prompt and image fidelity in texture editing.</p>
  <br>
  <img class="summary-img" src="./DreamBooth_files/5.png" style="width:100%;"> <br>
</div>
<div class="content">
  <h2>Plug and play adapter</h2>
  <p>Our model is an universal adapter because we freeze the original diffusion model in the training stage. It can generalize to the custom model checkpoints fine-tuned from the same base model. In Figure below, we verify this on community models from HuggingFace and CivitAi [8] including Realistic Vision V4.0 [2], ReV-Animated [31], Anything v4 [39] and Esthetic Retro Anime [22]. These models are all fine-tuned from SD v1.5. MoMA can be directly applied to these community models without any modification..</p>
  <br>
  <img class="summary-img" src="./DreamBooth_files/6.png" style="width:100%;"> <br>
</div>
<!-- <div class="content">
  <h2>Accessorization</h2>
  <p>Outfitting a dog with accessories. The identity of the subject is preserved and many different outfits or accessories can be applied to the dog given a prompt of type <em>"a [V] dog wearing a police/chef/witch outfit''</em>. We observe a realistic interaction between the subject dog and the outfits or accessories, as well as a large variety of possible options.</p>
  <br>
  <img class="summary-img" src="./DreamBooth_files/accessories.png" style="width:100%;"> <br>
</div> -->
<!-- <div class="content">
  <h2>Societal Impact</h2>
  <p>This project aims to provide users with an effective tool for synthesizing personal subjects (animals, objects) in different contexts. While general text-to-image models might be biased towards specific attributes when synthesizing images from text, our approach enables the user to get a better reconstruction of their desirable subjects. On contrary, malicious parties might try to use such images to mislead viewers. This is a common issue, existing in other generative models approaches or content manipulation techniques. Future research in generative modeling, and specifically of personalized generative priors, must continue investigating and revalidating these concerns.</p>
  <br>
</div> -->
<!-- <div class="content">
  <h2>BibTex</h2>
  <code> @article{ruiz2022dreambooth,<br>
  &nbsp;&nbsp;title={MoMA: new github page. test new branch},<br>
  &nbsp;&nbsp;author={Ruiz, Nataniel and Li, Yuanzhen and Jampani, Varun and Pritch, Yael and Rubinstein, Michael and Aberman, Kfir},<br>
  &nbsp;&nbsp;booktitle={arXiv preprint arxiv:2208.12242},<br>
  &nbsp;&nbsp;year={2022}<br>
  } </code> 
</div> -->
<!-- <div class="content" id="acknowledgements">
  <p><strong>Acknowledgements</strong>:
    We thank Rinon Gal, Adi Zicher, Ron Mokady, Bill Freeman, Dilip Krishnan, Huiwen Chang and Daniel Cohen-Or for their valuable inputs that helped improve this work, and to Mohammad Norouzi, Chitwan Saharia and William Chan for providing us with their support and the pretrained models of Imagen. Finally, a special thank you to David Salesin for his feedback, advice and for his support for the project.
    Recycling a familiar <a href="https://chail.github.io/latent-composition/">template</a> ;). --> 
  <!-- </p> -->
<!-- </div> -->
<br><br>
<footer class="footer">
  <div class="container">
    <div class="columns is-centered">                                                                                                                                   
      <!-- <div class="content"> -->
      The website template is taken from <a href="https://dreambooth.github.io/">dreambooth</a> project page.
      <!-- </div> -->
    </div>
  </div>
</footer>
<br><br>
</body>
</html>
 





