<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>MoMA Personalization</title>
<link href="./DreamBooth_files/style.css" rel="stylesheet">
<script type="text/javascript" src="./DreamBooth_files/jquery.mlens-1.0.min.js"></script> 
<script type="text/javascript" src="./DreamBooth_files/jquery.js"></script>
</head>

<body>
<div class="content">
  <!-- <h1>MoMA: Multimodal LLM Adapter for Fast Personalized Image Generation</h1> -->
  <h1><span style="color: red;">MoMA</span>: <strong><span style="color: red;">M</span>ultim<span style="color: red;">o</span>dal LL<span style="color: red;">M</span> <span style="color: red;">A</span>dapter for Fast Personalized</strong></h1>
  <h1><strong>Image Generation</strong></h1>
  <p id="authors"><span><a href="https://natanielruiz.github.io/"></a></span>Kunpeng Song<sup>1,</sup><sup>2</sup>,    Yizhe zhu<sup>1</sup>,   Bingchen Liu<sup>1</sup>,   Qing Yan<sup>1</sup>,    Ahmed Elgammal<sup>2</sup>    and Xiao Yang<sup>1</sup><br>
    <br>
  <span style="color: rgb(0, 85, 150);font-size: 20px" class="author-block"><sup>1</sup>ByteDance</span>
  <span style="color: rgb(0, 85, 150);font-size: 20px" class="author-block"><sup>2</sup>Rutgers University</span>
  </p>
  <br>
  <img src="./DreamBooth_files/hero_web.jpg" class="teaser-gif" style="width:100%;"><br>
    <font size="+2">
          <p style="text-align: center;">
            <a href="https://arxiv.org/abs/2404.05674" target="_blank">[Paper]</a> &nbsp;&nbsp;&nbsp;&nbsp;
            <a href="https://github.com/bytedance/MoMA/tree/main" target="_blank">[Code (New!!!)]</a> &nbsp;&nbsp;&nbsp;&nbsp;
            <a href="https://huggingface.co/KunpengSong/MoMA_llava_7b" target="_blank">[HuggingFace]</a> &nbsp;&nbsp;&nbsp;&nbsp;
          </p>
    </font>
</div>
<div class="content">
  <h2 style="text-align:center;">Abstract</h2>
  <p>In this paper, we present MoMA: an open-vocabulary, training-free personalized image model that boasts flexible zero-shot capabilities. As foundational text-to-image models rapidly evolve, the demand for robust image-to-image translation grows. Addressing this need, MoMA specializes in subject-driven personalized image generation. Utilizing an open-source, Multimodal Large Language Model (MLLM), we train MoMA to serve a dual role as both a feature extractor and a generator. This approach effectively synergizes reference image and text prompt information to produce valuable image features, facilitating an image diffusion model. To better leverage the generated features, we further introduce a novel self-attention shortcut method that efficiently transfers image features to an image diffusion model, improving the resemblance of the target object in generated images. Remarkably, as a tuning-free plug-and-play module, our model requires only a single reference image and outperforms existing methods in generating images with high detail fidelity, enhanced identity-preservation and prompt faithfulness. We commit to making our work open-source, thereby providing universal access to these advancements.</p>
</div>
<div class="content">
  <h2>Approach</h2>
  <p> We present MoMA, a multimodal LLM adapter enhanced by fine-grained feature transfer. The overall architecture is demonstrated in the Figure below. Our method consists of three parts: (1) a generative multimodal decoder is utilized to extract image features from the reference image and edit it following the target prompt, yielding the contextualized image feature; (2) we replace the background of the original image by white color, leaving only object pixels, and leverage the original UNetâ€™s self-attention layers to extract the object image feature; (3) finally, during the new image generation process, we injected the contextualized image features and the object image features into the UNet diffusion model with the dedicatedly trained context-cross-attention layers and object-cross-attention layers, respectively.</p>
  <br>
  <img class="summary-img" src="./DreamBooth_files/2.png" style="width:100%;"> <br>
  <p>To achieve the best model performance, we propose a two-staged pre-training strategy. First, we propose a Multimodal Generative Learning Stage, where we pre-train the multimodal image-feature decoder such that it learns to compose image features of the subject with the target prompt and output the CLIP embedding of the target image. Second, subject and context cross attention layers are trained to inject this embedding. To further enhance the detail faithfulness, we involve image self-attention features transfer and apply a masking mechanism</p>
  <br>
  <img class="summary-img" src="./DreamBooth_files/3.png" style="width:100%;"> <br>
</div>
<div class="content">
  <h2>Results for context editing</h2>
  <p>We present qualitative examples to illustrate the effectiveness of our model. In the Figure below, the target prompts specify a novel contextual environment. Our model seamlessly generates a high-quality background while precisely situating the same object within this new setting </p>
<img class="summary-img" src="./DreamBooth_files/4.png" style="width:100%;">
</div>
<div class="content">
  <h2>Results for texture editing</h2>
  <p>In the following image, the prompts indicate a change in texture. Our model showcases its ability to render realistic textures in response to the textual cues, adeptly altering specified visual elements while leaving other identity aspects of the image unaffected..</p>
  <br>
  <img class="summary-img" src="./DreamBooth_files/texture.png" style="width:100%;"> <br>
</div>
<div class="content">
  <h2>Compare with other methods</h2>
  <p>Zero-shot Qualitative Comparison. We share recontextualization in the upper panel and texture editing in the lower panel. Our results have significantly more accurate details for context editing and better balancing between prompt and image fidelity in texture editing.</p>
  <br>
  <img class="summary-img" src="./DreamBooth_files/5.png" style="width:100%;"> <br>
</div>
<div class="content">
  <h2>Plug and play adapter</h2>
  <p>Our model is a universal adapter because we freeze the original diffusion model in the training stage. It can generalize to the custom model checkpoints fine-tuned from the same base model. In Figure below, we verify this on community models from HuggingFace and CivitAi including Realistic Vision V4.0, ReV-Animated, Anything v4, and Esthetic Retro Anime. These models are all fine-tuned from SD v1.5. MoMA can be directly applied to these community models without any modification..</p>
  <br>
  <img class="summary-img" src="./DreamBooth_files/6.png" style="width:100%;"> <br>
</div>
<div class="content">
  <h2>BibTex</h2>
  <code> @article{song2024moma,<br>
  &nbsp;&nbsp;title={MoMA: Multimodal LLM Adapter for Fast Personalized Image Generation},<br>
  &nbsp;&nbsp;author={Song, Kunpeng and Zhu, Yizhe and Liu, Bingchen and Yan, Qing and Elgammal, Ahmed and Yang, Xiao},<br>
  &nbsp;&nbsp;booktitle={arXiv preprint arXiv:2404.05674},<br>
  &nbsp;&nbsp;year={2024}<br>
  } </code> 
</div>
<br><br>
<footer class="footer">
  <div class="container">
    <div class="columns is-centered">                                                                                                                                   
      <!-- <div class="content"> -->
      The website template is taken from <a href="https://dreambooth.github.io/">dreambooth</a> project page.
      <!-- </div> -->
    </div>
  </div>
</footer>
<br><br>
</body>
</html>
 





